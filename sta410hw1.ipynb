{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3aa0add",
   "metadata": {},
   "source": [
    "# STA410 Homework 1 (25 points)\n",
    "\n",
    "Welcome.\n",
    "\n",
    "## Rules\n",
    "\n",
    "0. Point awards for assigning the correct values into variables are indicated along with each required variable assignment name.\n",
    "\n",
    "1. **Do not delete or replace cells**: this erases cell ids upon which automated scoring is based.\n",
    "\n",
    "    - Cell ids are supported by [notebook format 4.5](https://github.com/jupyterlab/jupyterlab/issues/9729) or greater, and [jupyterlab](https://jupyter.org/install) version\n",
    "[3.0.13 or greater](https://github.com/jupyterlab/jupyterlab/releases/tag/v3.0.13). If the environment you work in does not support cell ids you will not get any credit for your submitted homework.  [UofT JupyterHub](https://jupyter.utoronto.ca) and [Google Colab](https://colab.research.google.com) support cell ids.\n",
    "      > You may check if cell ids are present or changing at each save with `! grep '\"id\":' <path/to/notebook>.ipynb`\n",
    "\n",
    "    - *You may add cells for scratch work*, but if required answers are not submitted through the provided cells where the answers are requested your answers will not be graded.\n",
    "    - *If you accidentally delete a required cell you can redownload the notebook* (so it has the correct required cells ids) and repopulate it with your answers (assuming you don't overwrite them).\n",
    "    \n",
    "2. **No cells may have any runtime errors**: this causes subsequent tests to fail and you will not get credit for tests which fail because of previous runtime errors.\n",
    "    - The `try`-`except` block syntax \"catches\" runtime errors and transforms them into `exceptions` which are no longer runtime errors.  These `exceptions` will not cause subsequent tests to fail.\n",
    "\n",
    "3. **No jupyter shortcut commands, e.g.,** `! python script.py 10` **may be used**: they will cause subsequent tests to fail.\n",
    "\n",
    "4. Specific code solutions submitted for these assignments must be created either individually or in the context of a paired effort.\n",
    "  \n",
    "  - Students may work individually.  \n",
    "    - Students choosing to work individually must work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity)  of \"honesty, trust, fairness, respect, responsibility and courage.\"\n",
    "  - Students may self-select pairs for each assignment.\n",
    "    - Paired students work together and may share work without restriction within their pair; but, must otherwise work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity) noted above.\n",
    "    - Paired students each separately submit their (common) work, including (agreeing) contribution of work statements for each problem.\n",
    "    \n",
    "    *Please seek homework partners in person or on the course discussion board on Quercus. Groups of three or more are not allowed; however, students are welcome to amicably seek new partners for each new assignment.* \n",
    "\n",
    "  ***Getting and sharing \"hints\" from other classmates is allowed; but, the eventual code creation work and submission must be your own individual or paired creation.***\n",
    "\n",
    "5. The homework is open book, open notes, open internet, etc.\n",
    "\n",
    "6. You may use any functions available from all library imports below; otherwise, you are expected to create your own Python functionality based on the Python stdlib (standard libary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e256d28",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# You may use any functions available from the following library imports\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48de6e",
   "metadata": {},
   "source": [
    "# Problem 0 (required)\n",
    "\n",
    "Are you working with a partner to complete this assignment?  \n",
    "- If not, assign  the value of `None` into the variable `Partner`.\n",
    "- If so, assign the name of the person you worked with into the variable `Partner`.\n",
    "    - Format the name as `\"<First Name> <Last Name>\"` as a `str` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78081a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required\n",
    "Partner = #None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482b3e6",
   "metadata": {},
   "source": [
    "What was your contribution in completing the code for this assignments problems? Assign one of the following into each of the `Problem_X` variables below.\n",
    "\n",
    "- `\"I worked alone\"`\n",
    "- `\"I contributed more than my partner\"`\n",
    "- `\"My partner and I contributed equally\"`\n",
    "- `\"I contributed less than my partner\"`\n",
    "- `\"I did not contribute\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589b6a8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required\n",
    "Problem_1 = #\"I worked alone\"\n",
    "Problem_2 = #\"I worked alone\"\n",
    "Problem_3 = #\"I worked alone\"\n",
    "Problem_4 = #\"I worked alone\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08adf26b",
   "metadata": {},
   "source": [
    "# Problem 1 (5 points)\n",
    "\n",
    "Complete the function `gram_schmidt(X, method)` which returns the orthonormalized vector of the lineraly independent vectors produced from the $m$ columns of `X` for both\n",
    "\n",
    "  - `method=\"modified\"` for the  \"Modified\" Gram-Schmidt method \n",
    "\n",
    "    > \\begin{align*}\n",
    "& \\text{for } (k = 0, ..., m-1) \\{ \\\\\n",
    "  & \\quad \\tilde x_k = x_k \\\\\n",
    "  & \\} \\\\\n",
    "  & \\tilde x_0 = \\tilde x_0 (\\tilde x_0^T \\tilde x_0^{\\,})^{-\\frac{1}{2}}\\\\\n",
    "& \\text{for } (k = 1, ..., m-1) \\{ \\\\\n",
    "& \\quad \\text{for } (j = k, ..., m-1) \\{ \\\\\n",
    "  & \\quad \\quad \\tilde x_j = \\tilde x_j - \\tilde x_{k-1}(\\tilde x^{T}_{j} \\tilde x_{k-1}) \\\\\n",
    "  & \\quad \\} \\\\\n",
    "  & \\quad \\tilde x_k = \\tilde x_k (\\tilde x_k^T \\tilde x_k^{\\,})^{-\\frac{1}{2}} \\\\\n",
    "  & \\} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "  - `method=\"classic\"` for the  \"Classical\" Gram-Schmidt method where the vectors are calculated sequentially as\n",
    "\n",
    "    \\begin{align*} \\tilde x_k = {} & \\left(x_k - \\sum_{j=0}^{k-1} \\tilde x_j (\\tilde x^{T}_{j} x_{k})  \\right) \\\\\n",
    "  \\tilde x_k = {} & \\tilde x_k (\\tilde x_k^T \\tilde x_k^{\\,})^{-\\frac{1}{2}} \n",
    "  \\end{align*}\n",
    "\n",
    "*This problem is inspired by the **\"Modified\" and \"Classical\" Gram-Schmidt Transformations** section of Chapter 5.3 **Matrix Factorization** on pages 219-221 of James E. Gentle's **Computational Statistics** textbook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e28ddd",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gram_schmidt(X, method):\n",
    "    \n",
    "    '''\n",
    "    Returns the column vectors of `X` normalized \n",
    "    with the Gram Schmidt method=<\"modified\"|\"classic\">\n",
    "    '''\n",
    "    \n",
    "    if method==\"modified\":\n",
    "        pass\n",
    "    elif method==\"classic\":\n",
    "        pass\n",
    "    else:\n",
    "        return \"I don't know that method.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecad38",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- If `x1` and `x2` are ***orthogonal*** then `x1.dot(x2)` equals $0$ when numerically accurate.\n",
    "- If `x1` is a ***normal vector*** then `x1.dot(x1)` equals $1$ when numerically accurate.\n",
    "- `X_ = X.copy()` creates a new element rather than a new variable name pointing to the original object.\n",
    "    - For an `np.array` object `X`, e.g., `X=np.ones((n,m))`, assigning `X_ = X` and then changing elements of `X_`, e.g., `X_[i,j]=0`, will also chainge `X`.\n",
    "    - For a `list`, e.g., `X=[1,2,3]`, then `X_=X[:]#=X[0:-1]=X.copy() ` is sufficient.\n",
    "        - This is because a \"slice\" of a list produces a new list object (but a \"slice\" of an `np.array` object does not produce a new `np.array` object.\n",
    "- Consider what happens to $\\sum_{j=0}^{k-1} \\tilde x_j (\\tilde x^{T}_{j} x_{k})$ if the magnitude of one of the summands is not comparable to the rest.\n",
    "- Note that, or confirm for yourself that `0==-0` evaluates to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207afbf",
   "metadata": {},
   "source": [
    "### Problem 1 Questions 1-2 (2 points)\n",
    "\n",
    "The `gram_schmidt` function will be tested for various specifications of `sigma` using\n",
    "\n",
    "```\n",
    "from scipy import stats\n",
    "np.random.seed(10)\n",
    "X = stats.norm(sigma).rvs(size=(10,10))\n",
    "```\n",
    "\n",
    "on the basis of `(np.abs(gram_schmidt(X, 'modified')-gram_schmidt(X, 'classic'))).sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033f684",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p1q1 = lambda X: #gram_schmidt(X, 'modified') # i.e., assign the 'modified' method to `p1q1`\n",
    "                                              # so `p1q1(X)` calls `gram_schmidt(X, 'modified')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e0a9f",
   "metadata": {},
   "source": [
    "## `lambda X: gram_schmidt(X, 'modified')`?\n",
    "\n",
    "- `f = lambda x: g(x)` is equivalent to\n",
    "\n",
    "```\n",
    "def f(x):\n",
    "  return g(x)\n",
    "```\n",
    "   \n",
    "- `lambda x: <do something with x>` is a so-called ***anonymous*** function; though, above it is assigned to the variable `f` as a shortcut for the full function definition syntax, so it is no longer ***anonymous***.\n",
    "\n",
    "- `sort(x, key = lambda x: -x)`, which is equivalent to `sort(x, reverse=True)` for numeric ***iterable*** `x`, is a common ***anonymous*** function use case in which the ***anonymous*** function is temporary and disappears after use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151571f3",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p1q2 = lambda X: #gram_schmidt(X, 'classic') # i.e., assign the 'classic' method to `p1q2`\n",
    "                                             # so `p1q2(X)` is `gram_schmidt(X, 'modified')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9a0bc",
   "metadata": {},
   "source": [
    "### Problem 1 Questions 3-4 (2 points)\n",
    "\n",
    "Order   \n",
    "\n",
    "- \"X\": `X = stats.norm(loc=1e10).rvs(size=(10,10))`\n",
    "- \"X_GSm\": `gram_schmidt(X, 'modified')` \n",
    "- \"X_GSc\": `gram_schmidt(X, 'classic')` \n",
    "\n",
    "by their ***condition*** (best to worst) under the problem `A.dot(b)` for `b=stats.norm(scale=1e-10).rvs(size=(10,1))` according to your observations of `A.dot(b)` as well as `np.linalg.cond(A)`; and, specify which of the following is the main cause of the difference in numerical tractibility between `gram_schmidt(X, 'modified')` and `gram_schmidt(X, 'classic')`:\n",
    "\n",
    "- \"They are computing different possible answers, one of which is better\"\n",
    "- \"The subtraction step in the <modified|classical> method is more accurate\"\n",
    "- \"The square root step in the <modified|classical> method is more accurate\"\n",
    "- \"The <modified|classical> method has less roundoff error since it has fewer total operations\"\n",
    "\n",
    "where either \"modified\" or \"classical\" replaces the placeholder <modified|classical>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84605a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p1q3 = #(<best>, <middle>, <worst>) # where \"X\", \"X_GSm\", \"X_GSc\" are placed in the correct placeholder position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7917f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p1q4 = #\"<replace this placeholder with the correct answer from the options above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f43caf",
   "metadata": {},
   "source": [
    "### Problem 1 Question 5 (1 point)\n",
    "\n",
    "Compute the following\n",
    "\n",
    "```\n",
    "np.random.seed(10)\n",
    "X = stats.norm(loc=1e5).rvs(size=(10,10))\n",
    "X_GSm_1e5 = gram_schmidt(X, 'modified')\n",
    "X_GSc_1e5 = gram_schmidt(X, 'classic')\n",
    "np.random.seed(10)\n",
    "X = stats.norm(loc=1e6).rvs(size=(10,10))\n",
    "X_GSm_1e6 = gram_schmidt(X, 'modified')\n",
    "X_GSc_1e6 = gram_schmidt(X, 'classic')\n",
    "```\n",
    "\n",
    "and for each orthogonalization `X_GS` examine \n",
    "\n",
    "`np.abs(np.round(X_GS.T.dot(X_GS),3))` \n",
    "\n",
    "and, for this level of ***precision***, determine how many of the four ***orthogonalizations*** fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4626e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p1q5 = #<1, 2, 3, or 4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74684ec5",
   "metadata": {},
   "source": [
    "# Problem 2 (7 points)\n",
    "\n",
    "Complete the function `python least_squares_fit_RSS(X, y)` that caclulates least squares model fits by \n",
    "\n",
    "  - inverting the gramian in support of the calculation $\\hat\\beta_{IG}=(X^TX)^{-1}X^Ty$ \n",
    "  - solving $R\\hat\\beta_{QR} = Q^Ty, \\;$ where $X=QR$ is the $QR$-*decomposition* of $X$\n",
    "\n",
    "and returns\n",
    "\n",
    "  - the RSS (residual sum of squares) produced by each of the model fits \n",
    "\n",
    "  - the time required to compute each of the model fits\n",
    "\n",
    "The mathematical equivalence of the two requested calculations follows because they both solve the same minimization problem:\n",
    "\n",
    "1. the familiar form of the (least squares) linear model fit $\\hat \\beta = (X^TX)^{-1}X^T y$ (for *full rank* $X$) is found using the *normal equations*:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat \\beta = {} & \\underset{\\quad \\; \\beta: \\; \\hat \\epsilon \\; = \\; X\\beta - y}{\\text{argmin } ||\\hat \\epsilon||_2^2} \n",
    "  = \\underset{\\beta}{\\text{argmin }} ||y-X \\beta||_2^2 \\\\\n",
    "  = {} & \\underset{\\beta}{\\text{argmin }} (y-X \\beta)^T(y- X \\beta) \n",
    "  = \\underset{\\beta}{\\text{argmin }} \\underbrace{\\beta^TX^TX \\beta -2\\beta^TX^Ty}_{f(\\beta)} \\\\\n",
    "  0 = {} & \\overbrace{\\nabla_\\beta \\left( \\beta^TX^TX \\beta -2\\beta^TX^Ty \\right)}^{\\nabla_\\beta f(\\hat \\beta )}\\\\\n",
    "  0 = {} &  X^TX \\hat \\beta - X^T y  \\quad \\Longrightarrow \\quad  \\underset{{\\text{$\\sum_i \\hat \\epsilon_i = 0$ if $X$ has an intercept column} }}{\\overset{\\text{residuals are othogonal to columns $X_{*j}$}}{0 = X^T (X \\hat \\beta - y) = X^T\\hat \\epsilon }} \\\\\n",
    "  \\Longrightarrow \\quad X^T y = {} &  X^TX \\hat \\beta \\quad \\longleftarrow \\quad \\textrm{the so-called } \\textit{normal equations}\n",
    "  \\end{align*}\n",
    "\n",
    "2. but, assuming $X$ is *full rank* (as above), $\\hat \\beta$ may be alternatively computed by solving the system of linear equations\n",
    "\n",
    "$$R \\beta = Q^Ty \\quad \\text{ which is } \\textit{uniquely consistent } \\quad \\text{ since $R^{-1}$ exists if $X$ is } \\textit{full rank}$$ \n",
    "\n",
    "where $X=QR$ is the $QR$-*decomposition* of $X$: \n",
    "\n",
    "\\begin{align*}\n",
    "  {} & \\underset{\\beta}{\\min \\;} (y - X \\beta)^T(y - X \\beta) \\\\\n",
    "  = {} & \\underset{\\beta}{\\min \\;}  \\left(y - [Q|Q']\\left[\\begin{array}{c}R\\\\0\\end{array}\\right] \\beta\\right)^T\\left(y - [Q|Q']\\left[\\begin{array}{c}R\\\\0\\end{array}\\right] \\beta\\right)\\\\\n",
    "  = {} & \\underset{\\beta}{\\min \\;} \\left(\\left[\\begin{array}{c}Q^T\\\\Q'^T\\end{array}\\right]y - \\left[\\begin{array}{c}R\\\\0\\end{array}\\right] \\beta\\right)^T\\left(\\left[\\begin{array}{c}Q^T\\\\Q'^T\\end{array}\\right]y - \\left[\\begin{array}{c}R\\\\0\\end{array}\\right] \\beta\\right) \\quad \\text{ since $\\left[\\begin{array}{c}Q^T\\\\Q'^T\\end{array}\\right][Q|Q']=I$} \\\\\n",
    "  = {} & \\underset{\\beta}{\\min \\;} (Q^Ty - R \\beta)^T(Q^Ty - R \\beta) + (Q'y)^TQ'y \\\\\n",
    "  = {} & \\underset{\\beta}{\\min \\;} \\underbrace{(Q^Ty - R \\beta)^T(Q^Ty - R \\beta)}_{\\text{which is minimized when } R \\beta = Q^Ty} \\\\\n",
    "  \\end{align*}\n",
    "\n",
    "*This problem is inspired by the **Least Squares with a Full Rank Coefficient Matrix** section (and sections leading up to it) in Chapter 5.6 **Overdetermined Systems; Least Squares** on pages 228-232 of James E. Gentle's **Computational Statistics** textbook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def least_squares_fit_RSS(X, y):\n",
    "    \n",
    "    '''\n",
    "    Computes RSS for a least squares fit of a linear model by\n",
    "    - inverting the gramian in the normal equations\n",
    "    - performing a QR-decomposition of X\n",
    "    \n",
    "    Returns the two RSS values and time taken to fit each model\n",
    "    '''\n",
    "    \n",
    "    # <complete and>\n",
    "    RSS_IG,time_IGG,RSS_QR,time_QR = [0]*4\n",
    "    # <replace above>\n",
    "    \n",
    "    return RSS_IG, time_IGG, RSS_QR, time_QR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f3ec8",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "Use the following functionality to complete this problem.\n",
    "- `X.T`\n",
    "- `X.T.dot(X)`\n",
    "- `np.linalg.inv`\n",
    "- `np.linalg.qr`\n",
    "- `np.linalg.solve`\n",
    "- `epsilon = (y-X.dot(betahat))`\n",
    "- `epsilon.T.dot(epsilon)`\n",
    "- `tic = time.perf_counter()`\n",
    "- `toc = time.perf_counter()`\n",
    "- `toc-tic`\n",
    "- `np.linalg.cond(X)`\n",
    "- `np.linalg.cond(X)**2`\n",
    "- `np.linalg.cond(X.T.dot(X))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d036052",
   "metadata": {},
   "source": [
    "## Problem 2 Questions 1-3 (3 points)\n",
    "\n",
    "The `least_squares_fit_RSS` function will be tested on data randomly generated as\n",
    "\n",
    "```\n",
    "X = np.ones((n,p)) + stats.norm.rvs(size=(n,p))*sigma\n",
    "y = np.ones((n,1))\n",
    "```\n",
    "\n",
    "for various choices of `n`, `p`, and `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79908741",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "p2q123 = lambda X,y: #least_squares_fit_RSS(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a5d23",
   "metadata": {},
   "source": [
    "### Problem 2 Question 4 (1 point)\n",
    "\n",
    "Which of the following is true?\n",
    "\n",
    "- \"The model fit based on inversion is faster\"\n",
    "- \"The model fit based QR decomposition is faster\"\n",
    "- \"Inversion is in a lower algorithmic complexity class than QR-decomposition\"\n",
    "- \"QR-decomposition is in a lower algorithmic complexity class than inversion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bead47",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p2q4 = #\"<replace this placeholder with the correct answer from the options above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2df2dc",
   "metadata": {},
   "source": [
    "### Problem 2 Question 5 (1 point)\n",
    "\n",
    "Why is there a difference in the time these two methods take to compute a least squares fits?\n",
    "\n",
    "- \"The QR-decomposition approach has two computational steps while the inversion approach has only one\"\n",
    "- \"The sizes of the inversion and QR-decomposition problems are different\"\n",
    "- \"Inversion is in a lower algorithmic complexity class than QR-decomposition\"\n",
    "- \"QR-decomposition is in a lower algorithmic complexity class than inversion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206618b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p2q5 = #\"<replace this placeholder with the correct answer from the options above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d3895",
   "metadata": {},
   "source": [
    "### Problem 2 Question 6 (1 point)\n",
    "\n",
    "Which of the following is true\n",
    "\n",
    "- \"The model fit based on inversion is more accurate\"\n",
    "- \"The model fit based QR decomposition is more accurate\"\n",
    "- \"Both model fits are sufficiently accurate\"\n",
    "- \"Neither model fits is sufficiently accurate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece27cee",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p2q6 = #\"<replace this placeholder with the correct answer from the options above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c6751",
   "metadata": {},
   "source": [
    "### Problem 2 Question 7 (1 point)\n",
    "\n",
    "What is driving numerical differences between these two model fitting approaches?\n",
    "\n",
    "- \"There is roundoff error as always, but the numerical differences are not sufficiently detrimental to cause concern\"\n",
    "- \"Numerical overflow in computing the gramian of X and numerical underflow when inverting the gramian of X\"\n",
    "- \"The computed condition number of the gramian of X is the square of the condition number of X\"\n",
    "- \"The computed condition number of the gramian of X is approximately the square of the condition number of X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc52f5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p2q7 = #\"<replace this placeholder with the correct answer from the options above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecad32d",
   "metadata": {},
   "source": [
    "# Problem 3 (5 points)\n",
    "\n",
    "Complete the function `cholesky(A, k)` which returns the Cholesky decomposition $U^{T}U^{\\,}$ of the matrix $A_{n \\times n}$ after the `k`$^{th}$ element of $U$ has been added in the \"*inner product*\" algorithm\n",
    "\n",
    "\\begin{align*}\n",
    "  & u_{00} = \\sqrt{a_{00}} && \\text{# first element: element \"0\" assigned}\\\\\n",
    "  & \\text{for }(j=1,...,n-1)\\{\\\\\n",
    "  & \\quad u_{0j} = \\frac{a_{0j}}{u_{00}} && \\text{# next $n-1$ element assignments}\\\\\n",
    "  & \\} \\text{ # $O(n)$}\\\\\n",
    "  & \\text{for }(i=1,...,n-1)\\{ && \\text{# element assignments alternate: }\\\\\n",
    "  & \\quad u_{ii} = (a_{ii}- \\sum_{k=0}^{i-1} u^2_{ki})^{\\frac{1}{2}} && \\quad \\text{# diagonal element assignments}\\\\\n",
    "  & \\quad \\text{for }(j=i+1,...,n-1)\\{\\\\\n",
    "  & \\quad \\quad u_{ij} = (a_{ij} - \\sum_{k=0}^{i-1} u_{ki}u_{kj})/u_{ii} && \\quad\\quad \\text{# off-diagonal element assignments}\\\\\n",
    "  & \\quad \\} \\\\\n",
    "  & \\} \\text{ # $O(n^3)$ }  \\\\\n",
    "  &  \\;\\;\\, \\text{# sum loop in two for loops}\n",
    "  \\end{align*}\n",
    "\n",
    "*This problem is inspired by the Algorithm 5.1 **Cholesky Factorization** in the **Cholesky Factorization** section of Chapter 5.3 **Matrix Factorization** on pages 218-219 of James E. Gentle's **Computational Statistics** textbook. [Errata Warning: the Cholesky decomposition is $A_{nn} = U_{nn}^TU_{nn}$ which is not clear from the $A_{nn} = L_{nn}U_{nn}$ specification Table 5.1 **Matrix Factorizations** on page 219; and the associated reference to page 216 is also wrong and should instead be to page 218.]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cholesky(A, k):\n",
    "    \n",
    "    '''\n",
    "    Partial Cholesky decomposition of A.\n",
    "    \n",
    "    Returns U_k.T.dot(U_k) where U_k=0 except\n",
    "    U_k[r,c]=U[r,c] for the first k+1 elements added to U\n",
    "    by the \"inner product\" algorithm producing `A==U.T.dot(U)`.\n",
    "    '''\n",
    "    \n",
    "    #<complete code>\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbc24f",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- Note that `k` is a zero-based index, which starts at $0$ so $0$ indicates the first element of the index.\n",
    "- Do not overwrite `i`, `j`, and `k` in `for` loops.\n",
    "- Algorithmic correctness can be confirmed with `np.linalg.cholesky`\n",
    "\n",
    "Here are some examples working with `np.array` objects that can be useful for coding up this decomposition.\n",
    "- `A = np.diag(np.ones(3))+0.1`\n",
    "- `A.shape`\n",
    "- `A[0:2,0]**2`\n",
    "- `(A[0:2,0]**2).sum()`\n",
    "- `A[0:2,0].dot(A[0:2,1])`\n",
    "- `U = 0*A`\n",
    "- `U[0,0] = (A[0,0])**0.5`\n",
    "- `U.T.dot(U)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc086098",
   "metadata": {},
   "source": [
    "## Problem 3 Questions 1-5 (5 points)\n",
    "\n",
    "The `cholesky` function will be tested on \n",
    "\n",
    "```\n",
    "n = 6\n",
    "A = np.diag(np.ones(n))+c\n",
    "```\n",
    "\n",
    "for various choices of `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7aa37f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p3q1 = lambda A: #cholesky(A, k=1)[1,0] # [1,0] is the element of A that will be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194e5f9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p3q2 = lambda A: #cholesky(A, k=1)[1,1] # [1,1] is the element of A that will be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed70b83",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p3q3 = lambda A: #cholesky(A, k=7)[2,2] # [2,2] is the element of A that will be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480cf7a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p3q4 = lambda A: #cholesky(A, k=13)[3,4] # [3,4] is the element of A that will be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628e0b1",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p3q5 = lambda A: #cholesky(A, k=17)[4,5] # [4,5] is the element of A that will be checked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb0e20",
   "metadata": {},
   "source": [
    "# Problem 4 (8 points)\n",
    "\n",
    "Complete the function `conjugate_gradient_descent(A, b, x0, k)` which returns $x^{(k)}$ from the iterative process \n",
    "\n",
    "\\begin{align*}\n",
    "  \\text{solving for }    \\quad & {}  x \\text{ in } Ax = b \\\\\n",
    "  \\text{by optimizing } \\quad & {} \\underset{x}{\\min} || b-Ax ||_{2}^{A^{-1}} =  \\underset{x}{\\min} f(x)\\\\\n",
    "  \\text{where } \\quad & {} f(x) = \\sqrt{(b-Ax)^T A^{-1}(b-Ax)}\n",
    "  \\end{align*}\n",
    "\n",
    "  with the ***conjugant gradient method*** given in the section [\"the resulting algorithm\"](https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm) from the wikipedia ***conjugant gradient method*** page and initialized with $x^{(0)}= $ `x0`.\n",
    "\n",
    "*This problem is inspired by the distinction between $A$-conjugacy and $A^2$-conjugacy which manifests in the context of Algorithm 5.3 **The Conjugate Gradient Method for Solving the Symmetric Positive Definite System $Ax=b$** in the **Conjugate Gradient Methods for Symmetric Poitive Definite Systems** section of Chapter 5.4 **Iterative Methods** on pages 223-225 of James E. Gentle's **Computational Statistics** textbook. [Errata Warning: the discussion on pages 223-225 implies Algorithm 5.3 is based on $A$-conjugacy when in fact it is based on $A^2$-conjugacy; and the superscripts in the equation at the top of page 224 are indexed incorrectly and should start from $\\alpha^{(0)}p^{(0)}$ rather than $\\alpha^{(1)}p^{(1)}$]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient_descent(A, b, x0, k):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5984f2",
   "metadata": {},
   "source": [
    "### Problem 4 Questions 1-6 (6 points)\n",
    "\n",
    "Define the `conjugate_gradient_descent` function as directed in the problem prompt.\n",
    "The function tested for accuracy on inputs such as\n",
    "\n",
    "```\n",
    "A = stats.norm.rvs(size=(d,d))\n",
    "A = A.T.dot(A) # positive definite\n",
    "b=np.ones((d,1))\n",
    "x0=np.ones((d,1))\n",
    "k=1\n",
    "```\n",
    "\n",
    "***No variable assignments are required: the functions will be tested directly.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cf442",
   "metadata": {},
   "source": [
    "### Problem 4 Question 7 (1 point)\n",
    "\n",
    "Which of the following generally evaluates to True for the conjugate gradient directions $p_j$ and $p_{j\\not =k}$ used in the `conjugate_gradient_descent` function?\n",
    "\n",
    "- \"A\": `np.isclose(p_k.T.dot(p_k))`\n",
    "- \"B\": `np.isclose(p_k.T.dot(A).dot(p_k))`\n",
    "- \"C\": `np.isclose(p_k.T.dot(A).dot(A).dot(p_k))`\n",
    "- \"D\": `np.isclose(p_k.T.dot(A**2).dot(p_k))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c539d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p4q7 = #\"<replace this placeholder with A, B, C, or D corresponding to the correct answer from the options above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b3770",
   "metadata": {},
   "source": [
    "### Problem 4 Question 8 (1 point)\n",
    "\n",
    "Which of the following doesn't stabilize after $d$ iterations of the `conjugate_gradient_descent` function for $A_{d\\times d}$?\n",
    "\n",
    "- \"A\": $x_{k+1} = x_k + \\alpha_kp_k$\n",
    "- \"B\": $r_{k+1} = r_k - \\alpha_k A p_k$\n",
    "- \"C\": $p_{k+1} = r_{k+1} + \\beta_kp_k$\n",
    "- \"D\": $\\beta_{k+1} = r_{k+1}^Tr_{k+1}/r_{k}^Tr_{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e6027",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p4q8 = #\"<replace this placeholder with A, B, C, or D corresponding to the correct answer from the options above>\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
